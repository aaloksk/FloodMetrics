Y <- sort(PPY$Freq, decreasing=FALSE) #Getting the data in ascending order
lenY <- length(Y) #Number of elements in Y
#Fitting empirical distibution
Fit2.emp <- gringorten(Y)
#Fitting Poisson Distribution in PPY data
poisfit <- fitdist(Y,'pois',method='mle')
#Get lamda parameter of poisson fit
lam <- as.numeric(poisfit$estimate[1])
#Get the cdf using the lamda par
Fit2.theo <- ppois(Y, lambda = lam)
#Calculation of dispersion index
PPY$num <- (PPY$Freq-lam)**2 #Numerator
dis_ind <- sum(PPY$num)/sum(PPY$Freq)
dis_ind
#Getting probabilities
Fit.theo <- pgpd(X, shape = shp, scale = scl, loc=new_min)
stnzz <- GenParPois(i, FldFl, Min_AP, thres)
Min_AP
?evmix::pgpd
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
devtools::document("FloodMetrics")
devtools::check("FloodMetrics")
#Load all functions in the package
devtools::load_all("FloodMetrics")
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
install("FloodMetrics")
library(FloodMetrics)
#Setting working directory
path <- 'C:\\Users\\Aalok\\OneDrive - lamar.edu\\0000000000Research\\Flooding_Risk\\Station1'
setwd(path)
#Output is Stn1_FilFLood in FldFl
FldFl <- read.csv('Stn1_FilFlood.csv')
i <- 1
percentle <- 0.75
# Calculate thres_cfs using the percentile
Thres_cfs <- quantile(FldFl$Qzpeak, percentle)
Min_AP <- Thres_cfs
# Define a sequence of thresholds for analysis
X_thres <- seq(0, 0.95, 0.05)
# Initialize an empty dataframe to store results
allthres <- data.frame()
# Loop through each threshold and compute results using GenParPois function
for (thres in X_thres){
stnzz <- GenParPois(i, FldFl, Min_AP, thres)
allthres <- rbind(allthres, stnzz)
}
import eval()
library(eva)
# Loop through each threshold and compute results using GenParPois function
for (thres in X_thres){
stnzz <- GenParPois(i, FldFl, Min_AP, thres)
allthres <- rbind(allthres, stnzz)
}
#Getting probabilities
Fit.theo <- eva::pgpd(X, shape = shp, scale = scl, loc=new_min)
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
devtools::document("FloodMetrics")
devtools::document("FloodMetrics")
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
devtools::document("FloodMetrics")
devtools::document("FloodMetrics")
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
devtools::document("FloodMetrics")
devtools::check("FloodMetrics")
#Load all functions in the package
devtools::load_all("FloodMetrics")
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
devtools::document("FloodMetrics")
devtools::check("FloodMetrics")
#Load all functions in the package
devtools::load_all("FloodMetrics")
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
install("FloodMetrics")
library(FloodMetrics)
#Setting working directory
path <- 'C:\\Users\\Aalok\\OneDrive - lamar.edu\\0000000000Research\\Flooding_Risk\\Station1'
setwd(path)
#Importing peak Streamflow
#Must have a column labeled peak_va
AnPkFl <- read.csv('Stn1_08041500.csv')
#Output is Stn1_FilFLood in FldFl
FldFl <- read.csv('Stn1_FilFlood.csv')
i <- 1
percentle <- 0.75
# Calculate thres_cfs using the percentile
Thres_cfs <- quantile(FldFl$Qzpeak, percentle)
Min_AP <- Thres_cfs
# Define a sequence of thresholds for analysis
X_thres <- seq(0, 0.95, 0.05)
# Initialize an empty dataframe to store results
allthres <- data.frame()
result_allthres <- analyzeFloodMetrics(FldFl, i, Min_AP, perc)
result_allthres <- analyzeFloodMetrics(FldFl, i, Min_AP, percentle)
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
devtools::document("FloodMetrics")
devtools::check("FloodMetrics")
#Load all functions in the package
devtools::load_all("FloodMetrics")
#Individually checking various functions of the library using the actual data
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
install("FloodMetrics")
library(FloodMetrics)
#Setting working directory
path <- 'C:\\Users\\Aalok\\OneDrive - lamar.edu\\0000000000Research\\Flooding_Risk\\Station1'
setwd(path)
#Output is Stn1_FilFLood in FldFl
FldFl <- read.csv('Stn1_FilFlood.csv')
i <- 1
percentle <- 0.75
# Calculate thres_cfs using the percentile
Thres_cfs <- quantile(FldFl$Qzpeak, percentle)
Min_AP <- Thres_cfs
result_allthres <- analyzeFloodMetrics(FldFl, i, Min_AP, percentle)
result_GEVpars <- GEVPars(result_allthres)
result_GEVpars
#Minimum Annual Precipitation for Annual Maximum Series.
Min_AP <- 397
result_allthres <- analyzeFloodMetrics(FldFl, i, Min_AP, percentle)
result_GEVpars <- GEVPars(result_allthres)
result_GEVpars
library(VineCopula)
#Setting working directory
path <- 'C:\\Users\\Aalok\\OneDrive - lamar.edu\\0000000000Research\\Flooding_Risk\\Station1'
setwd(path)
df <- read.csv('Stn1_Flood_over_thres.csv')
head(df)
#Time to peak
df$Tim2Pk <- as.integer(difftime(df$TPeak, df$Start_Date, units="days"))
head(df)
#Volume
X_Vol <- df$Volume
#DUration
X_Dur <- df$Duration
#Flood Peaks
X_Peaks <- df$QPeak
#Time to peak
X_time2peak <- df$Tim2Pk
#Fit Volume
fit_Vol <- candfit(X_Vol)
metricx <- function(obj){
llnam <- obj$distname
llest <- as.vector(obj$estimate) # coefficients
llsd <- as.vector(obj$sd) # std. errors
lllik <- as.vector(obj$loglik) #loglikelihood
llaic <- as.vector(obj$aic)  # AIC value
zz <- data.frame(llnam,llest[1],llest[2],llsd[1],llsd[2],lllik,llaic)
return(zz)
}
###########################################################################################
###########################################################################################
# Fit 5 candidate distributions and get their metrics
# normal, lognormal, weibull, gamma and logistic
# Use MLE for estimation
candfit <- function(X){
library('fitdistrplus')
library("actuar") # for loglogisitic function
zz <- data.frame()
X.dist <- fitdist(X,'norm',method='mle')
X.met <- metricx(X.dist)
zz <- rbind(zz,X.met)
X.dist <- fitdist(X,'lnorm',method='mle')
X.met <- metricx(X.dist)
zz <- rbind(zz,X.met)
X.dist <- fitdist(X,'weibull',method='mle')
X.met <- metricx(X.dist)
zz <- rbind(zz,X.met)
X.dist <- fitdist(X,'gamma',method='mle', lower = c(0, 0))
X.met <- metricx(X.dist)
zz <- rbind(zz,X.met)
X.dist <- fitdist(X,'llogis',method='mle')
X.met <- metricx(X.dist)
zz <- rbind(zz,X.met)
colnames(zz) <- c('Dist_name','shape','scale','SEshape','SEscale','Loglik','AIC')
return(zz)
}
###########################################################################################
###########################################################################################
#FIt exponential distribution
timefit <- function(X){
exp_fit <- fitdist(X,'exp',method='mle')
zz1 <- metricx(exp_fit)
gam_fit <- fitdist(X,'gamma',method='mle', lower = c(0, 0))
zz2 <- metricx(gam_fit)
zz <- rbind(zz1,zz2)
colnames(zz) <- c('Dist_name','shape','scale','SEshape','SEscale','Loglik','AIC')
return(zz)
}
###########################################################################################
###########################################################################################
# Function to perform bivariate empirical distribution
gringorten2D <- function(idx,X){
dat <- X
val <- X[idx,]
kk <- ifelse((dat[,1]<=val[1] & dat[,2]<=val[2]),1,0)
zz <- (sum(kk) - 0.12)/(nrow(X)+0.44)
return(zz)
}
###########################################################################################
#Fit Volume
fit_Vol <- candfit(X_Vol)
#Fit Duration
fit_Dur <- candfit(X_Dur)
#Select Best fit
#Volume
min_index <- which.min(fit_Vol$AIC) # Find the index of the row with the lowest value in column AIC
best_Vol <- fit_Vol[min_index, ] # Extract the row with the lowest value in column AIC
best_Vol
fit_Vol
#Duration
min_index <- which.min(fit_Dur$AIC) # Find the index of the row with the lowest value in column AIC
best_Dur <- fit_Dur[min_index, ] # Extract the row with the lowest value in column AIC
pVol <- plnorm(X_Vol,best_Vol$shape, best_Vol$scale)
pDur <- plnorm(X_Dur,best_Dur$shape, best_Dur$scale)
aa <- BiCopSelect(pVol,pDur,familyset=c(1:6))
#Create sequence of two variables
Vols <- seq(min(X_Vol), max(X_Vol), ((max(X_Vol)-min(X_Vol))/200))
Durs <- seq(min(X_Dur), max(X_Dur), ((max(X_Dur)-min(X_Dur))/200))
#Probabilities
pVols <- plnorm(Vols,best_Vol$shape, best_Vol$scale)
pDurs <- plnorm(Durs,best_Dur$shape, best_Dur$scale)
pars <- expand.grid(pVols,pDurs)
jprob <- BiCopCDF(pars[,1],pars[,2],family=5,par=0.38)
jpmat <- matrix(jprob,201,201,byrow=F)
contour(Vols,Durs,jpmat,levels=c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95),xlab='Volume',ylab='Duration',col='blue')
#Best fit of volume
shp <- 2.029116e+01
sc <- 6.620352e-01
pVol <- plnorm(X_Vol,meanlog=shp,sdlog=sc )
#Best fit of Duration
pDur <- pgamma(X_Dur,shape=5.28965687,scale=1/0.2727811)
# Joint Probability using 2D Gringorten
X <- cbind(X_Vol,X_Dur)
idx <- 1:nrow(X)
FX <- sapply(idx,gringorten2D,X)
FXX <- cbind(X,FX)
#Copula
aa <- BiCopSelect(pVol,pDur,familyset=c(1:9),rotations=TRUE)
jprob <- BiCopCDF(pVol,pDur,family=aa$family,par=aa$par,par2=aa$par2)
az <- cbind(FXX,jprob)
colnames(az) <- c('Vol','Dur','Empirical','Theoritical')
#PP Plot
plot(FX,jprob, xlab = 'Empirical Probability', ylab = 'Theoritical Probability', main = "VOl-Dur")
abline(coef = c(0,1), col='blue',lw=2)
#Storing copula information
zz <- data.frame(aa$familyname, aa$par,aa$par2, aa$tau, aa$logLik, aa$AIC)
colnames(zz) <- c('FamilyName','Parameter','Parameter2','Tau','LogLik','AIC')
zz
#Create sequence of two variables
Vols <- seq(min(X_Vol), max(X_Vol), ((max(X_Vol)-min(X_Vol))/200))
Durs <- seq(min(X_Dur), max(X_Dur), ((max(X_Dur)-min(X_Dur))/200))
pVols <- plnorm(Vols,meanlog=shp,sdlog=sc )
pDurs <- pgamma(Durs,shape=5.28965687,scale=1/0.2727811)
pars <- expand.grid(pVols,pDurs)
jprob <- BiCopCDF(pVols,pDurs,family=16,par=1.49)
jpmat <- matrix(jprob,201,201,byrow=F)
contour(Vols,Durs,jpmat,levels=c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95),xlab='TDS (mg/L)',ylab='SAR',col='blue')
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
devtools::document("FloodMetrics")
devtools::check("FloodMetrics")
#Load all functions in the package
devtools::load_all("FloodMetrics")
#3.2 Bivariate Copula Analysis with Best Fit Distributions
usethis::use_r("bivariateCopulaWithBestFit")
#Calling necessary libraries
library(devtools)
library(usethis)
#3.2 Bivariate Copula Analysis with Best Fit Distributions
usethis::use_r("bivariateCopulaWithBestFit")
#Setting working directory
path <- 'C:\\Users\\Aalok\\OneDrive - lamar.edu\\0000000000Research\\Flooding_Risk\\Station1'
setwd(path)
df <- read.csv('Stn1_Flood_over_thres.csv')
head(df)
#Time to peak
df$Tim2Pk <- as.integer(difftime(df$TPeak, df$Start_Date, units="days"))
head(df)
#Volume
X_Vol <- df$Volume
#DUration
X_Dur <- df$Duration
X1 <- X_Vol
X2 <- X_Dur
# Fit X1 and X2
fit_X1 <- fitMultipleDistributions(X1)
fitMultipleDistributions <- function(X) {
zz <- data.frame()
# Fit Normal Distribution
X.dist <- fitdist(X, 'norm', method = 'mle')
X.met <- extractFitStatistics(X.dist)
zz <- rbind(zz, X.met)
# Fit Lognormal Distribution
X.dist <- fitdist(X, 'lnorm', method = 'mle')
X.met <- extractFitStatistics(X.dist)
zz <- rbind(zz, X.met)
# Fit Weibull Distribution
X.dist <- fitdist(X, 'weibull', method = 'mle')
X.met <- extractFitStatistics(X.dist)
zz <- rbind(zz, X.met)
# Fit Gamma Distribution
X.dist <- fitdist(X, 'gamma', method = 'mle', lower = c(0, 0))
X.met <- extractFitStatistics(X.dist)
zz <- rbind(zz, X.met)
# Fit Logistic Distribution
X.dist <- fitdist(X, 'llogis', method = 'mle')
X.met <- extractFitStatistics(X.dist)
zz <- rbind(zz, X.met)
# Set column names for the result
colnames(zz) <- c('Dist_name', 'shape', 'scale', 'SEshape', 'SEscale', 'Loglik', 'AIC')
return(zz)
}
# Fit X1 and X2
fit_X1 <- fitMultipleDistributions(X1)
library("fitdistrplus")
library("lfstat")
library("extRemes")
library("evmix")
library("SpatialExtremes")
library('VGAM')  #For pgpd
library("nortest")
library(lubridate)
# Fit X1 and X2
fit_X1 <- fitMultipleDistributions(X1)
extractFitStatistics <- function(obj) {
dist_name <- obj$distname
estimates <- as.vector(obj$estimate) # coefficients
std_errors <- as.vector(obj$sd) # standard errors
log_likelihood <- as.vector(obj$loglik) # log-likelihood
aic_value <- as.vector(obj$aic)  # AIC value
result <- data.frame(
Distribution = dist_name,
Shape = estimates[1],
Scale = estimates[2],
SDShape = std_errors[1],
SDScale = std_errors[2],
LogLikelihood = log_likelihood,
AIC = aic_value
)
return(result)
}
# Fit X1 and X2
fit_X1 <- fitMultipleDistributions(X1)
import(actuar)
library('actuar')
# Fit X1 and X2
fit_X1 <- fitMultipleDistributions(X1)
fit_X2 <- fitMultipleDistributions(X2)
# Select Best fit for X1
min_index_X1 <- which.min(fit_X1$AIC)
best_X1 <- fit_X1[min_index_X1, ]
# Select Best fit for X2
min_index_X2 <- which.min(fit_X2$AIC)
best_X2 <- fit_X2[min_index_X2, ]
# Get CDF values based on best fit
pX1 <- cumulativeDensityFunction(best_X1$Dist_name, X1, best_X1$shape, best_X1$scale)
cumulativeDensityFunction <- function(dbn, P, shp, sc) {
if (!is.character(dbn) || !(dbn %in% c("norm", "lnorm", "gamma", "weibull", "llogis"))) {
stop("Invalid distribution specified.")
}
if (dbn == "norm") {
pP = pnorm(P, mean = shp, sd = sc)
} else if (dbn == "lnorm") {
pP = plnorm(P, meanlog = shp, sdlog = sc)
} else if (dbn == "gamma") {
pP = pgamma(P, shape = shp, scale = 1/sc)
} else if (dbn == "weibull") {
pP = pweibull(P, shape = shp, scale = sc)
} else if (dbn == "llogis") {
pP = pllogis(P, shape = shp, scale = sc)
}
return(pP)
}
# Get CDF values based on best fit
pX1 <- cumulativeDensityFunction(best_X1$Dist_name, X1, best_X1$shape, best_X1$scale)
pX2 <- cumulativeDensityFunction(best_X2$Dist_name, X2, best_X2$shape, best_X2$scale)
# Joint Probability using 2D Gringorten
X <- cbind(X1, X2)
idx <- 1:nrow(X)
FX <- sapply(idx, gringorten2D, X)
library(VineCopula)
gringorten2D <- function(idx,X){
dat <- X
val <- X[idx,]
kk <- ifelse((dat[,1]<=val[1] & dat[,2]<=val[2]),1,0)
zz <- (sum(kk) - 0.12)/(nrow(X)+0.44)
return(zz)
}
FX <- sapply(idx, gringorten2D, X)
FXX <- cbind(X, FX)
# Copula
aa <- BiCopSelect(pX1, pX2, familyset = c(1:9), rotations = TRUE)
jprob <- BiCopCDF(pX1, pX2, family = aa$family, par = aa$par, par2 = aa$par2)
az <- cbind(FXX, jprob)
colnames(az) <- c('X1', 'X2', 'Empirical', 'Theoretical')
# PP Plot
plot(FX, jprob, xlab = 'Empirical Probability', ylab = 'Theoretical Probability', main = "Bivariate Analysis")
abline(coef = c(0, 1), col = 'blue', lw = 2)
# Storing copula information
zz <- data.frame(aa$familyname, aa$par, aa$par2, aa$tau, aa$logLik, aa$AIC)
colnames(zz) <- c('FamilyName', 'Parameter', 'Parameter2', 'Tau', 'LogLik', 'AIC')
# Create sequence of two variables
seqX1 <- seq(min(X1), max(X1), length.out = 201)
seqX2 <- seq(min(X2), max(X2), length.out = 201)
pSeqX1 <- cumulativeDensityFunction(best_X1$Dist_name, seqX1, best_X1$shape, best_X1$scale)
pSeqX2 <- cumulativeDensityFunction(best_X2$Dist_name, seqX2, best_X2$shape, best_X2$scale)
pars <- expand.grid(pSeqX1, pSeqX2)
jprob_grid <- BiCopCDF(pars[, 1], pars[, 2], family = aa$family, par = aa$par, par2 = aa$par2)
jpmat <- matrix(jprob_grid, 201, 201, byrow = FALSE)
contour(seqX1, seqX2, jpmat, levels = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95),
xlab = 'X1', ylab = 'X2', col = 'blue')
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
install("FloodMetrics")
library(FloodMetrics)
#Setting working directory
path <- 'C:\\Users\\Aalok\\OneDrive - lamar.edu\\0000000000Research\\Flooding_Risk\\Station1'
setwd(path)
#Getting Floods Characteristics
floodq <- read.csv('Stn1_FloodQ.csv')
zz <- floodCharacteristics(floodq, '08041500')
zz
bivariateCopulaWithBestFit(zz$Volume, zz$Duration)
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
devtools::document("FloodMetrics")
devtools::check("FloodMetrics")
devtools::document("FloodMetrics")
devtools::document("FloodMetrics")
devtools::check("FloodMetrics")
install.packages(copula)
devtools::document("FloodMetrics")
devtools::document("FloodMetrics")
devtools::check("FloodMetrics")
#Load all functions in the package
devtools::load_all("FloodMetrics")
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
install("FloodMetrics")
library(FloodMetrics)
#Setting working directory
path <- 'C:\\Users\\Aalok\\OneDrive - lamar.edu\\0000000000Research\\Flooding_Risk\\Station1'
setwd(path)
#Daily flow
DlFl <- read.csv('Stn1DF.csv')
#Getting Floods Characteristics
floodq <- read.csv('Stn1_FloodQ.csv')
zz <- floodCharacteristics(floodq, '08041500')
bivariateCopulaWithBestFit(zz$Volume, zz$Duration)
library (actuar)
bivariateCopulaWithBestFit(zz$Volume, zz$Duration)
zz$Volume
#Daily flow
DlFl <- read.csv('Stn1DF.csv')
, start = "10/1/1939", end = "9/30/2022"
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
install("FloodMetrics")
library(FloodMetrics)
#Setting working directory
path <- 'C:\\Users\\Aalok\\OneDrive - lamar.edu\\0000000000Research\\Flooding_Risk\\Station1'
setwd(path)
#Daily flow
DlFl <- read.csv('Stn1DF.csv')          ########################################################################
tail(DlFl)
DlPF <- separateBaseflowAndFloodflow(DlFl, start = "10/1/1939", end = "9/30/2022")
DlPF
#ExtractPeakFlow
DL_PeakQT <- extractPeakFlow(DlPF)
DL_PeakQT
Ar <- 2228.481                           ########################################################################
independenceCriteriaCheck(DL_PeakQT, Ar, DlFl)
independenceCriteriaCheck(DL_PeakQT, Ar, DlFl)
DL_IndCk <- independenceCriteriaCheck(DL_PeakQT, Ar, DlFl)
DL_IndCk <- independenceCriteriaCheck(DL_PeakQT, Ar, DlFl)
warnings()
#Calling necessary libraries
library(devtools)
library(usethis)
setwd("C:/Users/Aalok/OneDrive - lamar.edu/0000000000Research/Flooding_Risk/R_Package")
devtools::document("FloodMetrics")
devtools::check("FloodMetrics")
#Load all functions in the package
devtools::load_all("FloodMetrics")
devtools::build_manual()
devtools::build_manual("FloodMetrics")
options(error = recover)
devtools::build_manual("FloodMetrics")
